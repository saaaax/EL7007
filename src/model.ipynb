{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ff159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import log_softmax\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from jiwer import wer, cer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f97ab",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5538a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # reduce H y W\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, None))  # fuerza H = 1, W se mantiene\n",
    "        )\n",
    "\n",
    "        # Supongamos que la salida del CNN tiene tamaño (B, 128, 1, W)\n",
    "        # y por tanto la entrada al RNN será (W, B, 128)\n",
    "        self.rnn = nn.LSTM(input_size=128, hidden_size=256, num_layers=2, bidirectional=True)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)               # (B, 128, H=1, W)\n",
    "        conv = conv.squeeze(2)           # (B, 128, W)\n",
    "        conv = conv.permute(2, 0, 1)     # (W, B, 128)\n",
    "        rnn_out, _ = self.rnn(conv)      # (W, B, 512)\n",
    "        out = self.fc(rnn_out)           # (W, B, num_classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbde41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55387c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "CHARACTERS = \"-abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "char_to_idx = {c: i for i, c in enumerate(CHARACTERS)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir):\n",
    "        df = pd.read_csv(csv_file, header=None)\n",
    "        self.samples = df.values.tolist()\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((32, 128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        label = str(label)\n",
    "        img = Image.open(f\"{self.img_dir}/{img_path.split('/')[-1]}\").convert('L')\n",
    "        img = self.transform(img)\n",
    "        label_idx = [char_to_idx[c] for c in label if c in char_to_idx]\n",
    "        return img, torch.tensor(label_idx, dtype=torch.long), len(label_idx)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, labels, label_lengths = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    labels = torch.cat(labels)\n",
    "    return imgs, labels, torch.tensor(label_lengths)\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = OCRDataset(\"ocr_dataset_train/labels.csv\", \"ocr_dataset_train/img\")\n",
    "\n",
    "# Define the sizes for training and validation\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f306d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = CRNN(num_classes=len(CHARACTERS)).to(device)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9c76bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2500/2500 [18:50<00:00,  2.21batch/s, loss=0.658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 1.9705, Val Loss: 0.5882\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2500/2500 [02:46<00:00, 14.98batch/s, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.4291, Val Loss: 0.3473\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2500/2500 [02:45<00:00, 15.08batch/s, loss=0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.3128, Val Loss: 0.2930\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2500/2500 [02:45<00:00, 15.14batch/s, loss=0.274] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.2721, Val Loss: 0.2660\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2500/2500 [02:45<00:00, 15.13batch/s, loss=0.239] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.2496, Val Loss: 0.2465\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2500/2500 [02:46<00:00, 14.98batch/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 0.2351, Val Loss: 0.2398\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2500/2500 [02:47<00:00, 14.92batch/s, loss=0.263] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 0.2241, Val Loss: 0.2391\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2500/2500 [02:45<00:00, 15.09batch/s, loss=0.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 0.2152, Val Loss: 0.2259\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2500/2500 [02:45<00:00, 15.13batch/s, loss=0.131] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 0.2084, Val Loss: 0.2187\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2500/2500 [02:45<00:00, 15.14batch/s, loss=0.236] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 0.2025, Val Loss: 0.2184\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 2500/2500 [02:45<00:00, 15.07batch/s, loss=0.232] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train Loss: 0.1966, Val Loss: 0.2197\n",
      "No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 2500/2500 [06:51<00:00,  6.08batch/s, loss=0.218]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train Loss: 0.1914, Val Loss: 0.2179\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 2500/2500 [07:48<00:00,  5.34batch/s, loss=0.219]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train Loss: 0.1850, Val Loss: 0.2171\n",
      "Saved new best model to 'ocr_model_gamma.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 2500/2500 [02:56<00:00, 14.20batch/s, loss=0.0833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train Loss: 0.1791, Val Loss: 0.2221\n",
      "No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 2500/2500 [02:55<00:00, 14.25batch/s, loss=0.177] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train Loss: 0.1727, Val Loss: 0.2193\n",
      "No improvement for 2 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 2500/2500 [02:50<00:00, 14.69batch/s, loss=0.135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Train Loss: 0.1659, Val Loss: 0.2246\n",
      "No improvement for 3 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 2500/2500 [02:46<00:00, 15.01batch/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Train Loss: 0.1587, Val Loss: 0.2258\n",
      "No improvement for 4 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 2500/2500 [02:47<00:00, 14.90batch/s, loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Train Loss: 0.1508, Val Loss: 0.2356\n",
      "No improvement for 5 epoch(s)\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# Early stopping params\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "best_model_path = \"ocr_model_gamma.pth\"\n",
    "\n",
    "if train:\n",
    "    for epoch in range(30):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for imgs, labels, label_lengths in tepoch:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                logits = model(imgs)  # shape: (T, N, C)\n",
    "                log_probs = logits.log_softmax(2)\n",
    "                input_lengths = torch.full(size=(logits.size(1),), fill_value=logits.size(0), dtype=torch.long)\n",
    "\n",
    "                loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels, label_lengths in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                logits = model(imgs)\n",
    "                log_probs = logits.log_softmax(2)\n",
    "                input_lengths = torch.full(size=(logits.size(1),), fill_value=logits.size(0), dtype=torch.long)\n",
    "\n",
    "                loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ---- Early Stopping Check ----\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Saved new best model to '{best_model_path}'\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2290b2",
   "metadata": {},
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ad65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\AppData\\Local\\Temp\\ipykernel_1516\\2256506409.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"ocr_model_gamma.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): AdaptiveAvgPool2d(output_size=(1, None))\n",
       "  )\n",
       "  (rnn): LSTM(128, 256, num_layers=2, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CRNN(num_classes=len(CHARACTERS)).to(device)\n",
    "model.load_state_dict(torch.load(\"ocr_model_gamma.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c612eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((32, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"L\")\n",
    "    return transform(image).unsqueeze(0).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a106bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prediction(logits, idx_to_char):\n",
    "    # Logits are of shape (T, B, C) where T is sequence length, B is batch size, and C is number of classes\n",
    "    pred = logits.argmax(dim=2)  # (T, B)\n",
    "    pred = pred.permute(1, 0)    # (B, T) to (B, T)\n",
    "\n",
    "    # Convert indices to characters using idx_to_char mapping\n",
    "    decoded_texts = []\n",
    "    for i in range(pred.size(0)):  # Iterate over batch size\n",
    "        decoded_text = ''.join([idx_to_char[idx.item()] for idx in pred[i] if idx.item() != 0])  # Assuming 0 is the padding token\n",
    "        decoded_texts.append(decoded_text)\n",
    "    return decoded_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0827d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto detectado: ['ital']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_tensor = load_image(\"D:/my_files/universidad/intro_imagenes/EL7007/src/ocr_dataset_train/img/00002.png\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(image_tensor)  # (T, B, C)\n",
    "    logits = logits.log_softmax(2)\n",
    "\n",
    "predicted_texts = decode_prediction(logits, idx_to_char)\n",
    "print(\"Texto detectado:\", predicted_texts)\n",
    "\n",
    "img_np = image_tensor.squeeze().cpu().numpy()  # -> (H, W)\n",
    "# plt.imshow(img_np, cmap='gray')\n",
    "\n",
    "plt.imshow(img_np, cmap='gray')\n",
    "plt.title(f\"Texto detectado: {predicted_texts[0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0a00334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_binarize_image(path, threshold=100):\n",
    "    # Cargar imagen y convertir a escala de grises\n",
    "    image = Image.open(path).convert(\"L\")  # Modo \"L\" es escala de grises\n",
    "\n",
    "    # Convertir a numpy array\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Invertir si el fondo es negro y texto blanco (opcional)\n",
    "    image_np = 255 - image_np\n",
    "\n",
    "    # Aplicar umbral: letras negras (0), fondo blanco (255)\n",
    "    binarized = (image_np < threshold).astype(np.uint8) * 255\n",
    "\n",
    "    # Convertir de nuevo a tensor\n",
    "    image_tensor = transforms.ToTensor()(Image.fromarray(binarized))  # (1, H, W)\n",
    "    return image_tensor.unsqueeze(0)  # Añadir batch -> (1, 1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be603f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto detectado: ['corrtte']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path = \"D:/my_files/universidad/intro_imagenes/EL7007/src/ocr_dataset_test/corte.png\"\n",
    "image_tensor = load_and_binarize_image(image_path).cuda()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    logits = model(image_tensor)\n",
    "    logits = logits.log_softmax(2)\n",
    "\n",
    "predicted_texts = decode_prediction(logits, idx_to_char)\n",
    "print(\"Texto detectado:\", predicted_texts)\n",
    "\n",
    "# Load original image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert image_tensor to CPU and NumPy array for display\n",
    "# Assumes image_tensor shape is (1, C, H, W)\n",
    "tensor_image = image_tensor.squeeze().detach().cpu()  # shape: (C, H, W) or (H, W)\n",
    "if tensor_image.ndim == 3:\n",
    "    tensor_image = tensor_image.permute(1, 2, 0)  # (H, W, C)\n",
    "\n",
    "# Plot original and tensor image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Tensor image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(tensor_image, cmap='gray' if tensor_image.ndim == 2 or tensor_image.shape[2] == 1 else None)\n",
    "plt.title(f\"Detected: {predicted_texts[0]}\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9df2f",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8592cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = OCRDataset(\"ocr_dataset_test/labels.csv\", \"ocr_dataset_/img\")\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b30c515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader, device, idx_to_char):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, label_lengths in tqdm(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)  # [T, B, C]\n",
    "            log_probs = log_softmax(logits, dim=2)\n",
    "\n",
    "            # Predicción y decodificación\n",
    "            pred_texts = decode_prediction(log_probs, idx_to_char)\n",
    "\n",
    "            # Reconstrucción de etiquetas reales\n",
    "            labels = labels.cpu().tolist()\n",
    "            lengths = label_lengths.cpu().tolist()\n",
    "\n",
    "            gt_texts = []\n",
    "            i = 0\n",
    "            for l in lengths:\n",
    "                gt_texts.append(\"\".join([idx_to_char.get(c, \"\") for c in labels[i:i+l]]))\n",
    "                i += l\n",
    "\n",
    "            for pred, gt in zip(pred_texts, gt_texts):\n",
    "                if pred == gt:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be11c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_accuracy = calculate_accuracy(model, val_loader, device, idx_to_char)\n",
    "# print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f4ff6",
   "metadata": {},
   "source": [
    "### WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bee9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer(model, dataloader, device, idx_to_char):\n",
    "    model.eval()\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, label_lengths in tqdm(dataloader, desc=\"Calculating WER\"):\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            logits = logits.log_softmax(2)\n",
    "\n",
    "            pred_texts = decode_prediction(logits, idx_to_char)\n",
    "\n",
    "            label_offset = 0\n",
    "            for length in label_lengths:\n",
    "                true_text = ''.join([idx_to_char[idx.item()] for idx in labels[label_offset:label_offset + length]])\n",
    "                ground_truths.append(true_text)\n",
    "                label_offset += length\n",
    "\n",
    "            predictions.extend(pred_texts)\n",
    "\n",
    "    return wer(ground_truths, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca2a6e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating WER: 100%|██████████| 625/625 [01:33<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation WER (Word Error Rate): 84.09%\n"
     ]
    }
   ],
   "source": [
    "val_wer = calculate_wer(model, val_loader, device, idx_to_char)\n",
    "print(f\"Validation WER (Word Error Rate): {val_wer * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdc261",
   "metadata": {},
   "source": [
    "### CER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6badecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cer(model, dataloader, device, idx_to_char):\n",
    "    model.eval()\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, label_lengths in tqdm(dataloader, desc=\"Evaluando CER\"):\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            logits = logits.log_softmax(2)\n",
    "\n",
    "            pred_texts = decode_prediction(logits, idx_to_char)\n",
    "\n",
    "            label_offset = 0\n",
    "            for length in label_lengths:\n",
    "                true_text = ''.join([idx_to_char[idx.item()] for idx in labels[label_offset:label_offset + length]])\n",
    "                ground_truths.append(true_text)\n",
    "                label_offset += length\n",
    "\n",
    "            predictions.extend(pred_texts)\n",
    "\n",
    "    return cer(ground_truths, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b36c97b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando CER: 100%|██████████| 625/625 [01:38<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER (Character Error Rate): 33.55%\n"
     ]
    }
   ],
   "source": [
    "val_cer = calculate_cer(model, val_loader, device, idx_to_char)\n",
    "print(f\"Validation CER (Character Error Rate): {val_cer * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
