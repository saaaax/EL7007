{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3ff159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import log_softmax\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from jiwer import wer, cer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f97ab",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5538a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        resnet = resnet18(pretrained=True)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, kernel_size=1),  # convert 1-channel to 3-channel\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            nn.AdaptiveAvgPool2d((1, None))  # force height = 1\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=128, hidden_size=256, num_layers=2, bidirectional=True)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)               # (B, 128, 1, W)\n",
    "        conv = conv.squeeze(2)           # (B, 128, W)\n",
    "        conv = conv.permute(2, 0, 1)     # (W, B, 128)\n",
    "        rnn_out, _ = self.rnn(conv)      # (W, B, 512)\n",
    "        out = self.fc(rnn_out)           # (W, B, num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbde41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55387c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "CHARACTERS = \"-abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "char_to_idx = {c: i for i, c in enumerate(CHARACTERS)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir):\n",
    "        df = pd.read_csv(csv_file, header=None)\n",
    "        self.samples = df.values.tolist()\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((32, 128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        label = str(label)\n",
    "        img = Image.open(f\"{self.img_dir}/{img_path.split('/')[-1]}\").convert('L')\n",
    "        img = self.transform(img)\n",
    "        label_idx = [char_to_idx[c] for c in label if c in char_to_idx]\n",
    "        return img, torch.tensor(label_idx, dtype=torch.long), len(label_idx)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, labels, label_lengths = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    labels = torch.cat(labels)\n",
    "    return imgs, labels, torch.tensor(label_lengths)\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = OCRDataset(\"ocr_dataset_train/labels.csv\", \"ocr_dataset_train/img\")\n",
    "\n",
    "# Define the sizes for training and validation\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f306d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ignac\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ignac\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = CRNN(num_classes=len(CHARACTERS)).to(device)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9c76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping params\n",
    "early_stopping = False  # Set to False to disable early stopping\n",
    "patience = 10 \n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "best_model_path = \"ocr_model_delta.pth\"\n",
    "num_epochs = 30\n",
    "\n",
    "if train:\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for imgs, labels, label_lengths in tepoch:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                logits = model(imgs)  # shape: (T, N, C)\n",
    "                log_probs = logits.log_softmax(2)\n",
    "                input_lengths = torch.full(size=(logits.size(1),), fill_value=logits.size(0), dtype=torch.long)\n",
    "\n",
    "                loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels, label_lengths in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                logits = model(imgs)\n",
    "                log_probs = logits.log_softmax(2)\n",
    "                input_lengths = torch.full(size=(logits.size(1),), fill_value=logits.size(0), dtype=torch.long)\n",
    "\n",
    "                loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ---- Model Saving ----\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Saved new best model to '{best_model_path}'\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        # Save last epoch weights if early stopping is disabled\n",
    "        if not early_stopping and epoch == num_epochs - 1:  \n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Saved final epoch weights to '{best_model_path}'\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopping and epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2290b2",
   "metadata": {},
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ad65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\AppData\\Local\\Temp\\ipykernel_42624\\3849697122.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"ocr_model_delta.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): AdaptiveAvgPool2d(output_size=(1, None))\n",
       "  )\n",
       "  (rnn): LSTM(128, 256, num_layers=2, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CRNN(num_classes=len(CHARACTERS)).to(device)\n",
    "model.load_state_dict(torch.load(\"ocr_model_delta.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c612eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((32, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"L\")\n",
    "    return transform(image).unsqueeze(0).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a106bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prediction(logits, idx_to_char):\n",
    "    # Logits are of shape (T, B, C) where T is sequence length, B is batch size, and C is number of classes\n",
    "    pred = logits.argmax(dim=2)  # (T, B)\n",
    "    pred = pred.permute(1, 0)    # (B, T) to (B, T)\n",
    "\n",
    "    # Convert indices to characters using idx_to_char mapping\n",
    "    decoded_texts = []\n",
    "    for i in range(pred.size(0)):  # Iterate over batch size\n",
    "        decoded_text = ''.join([idx_to_char[idx.item()] for idx in pred[i] if idx.item() != 0])  # Assuming 0 is the padding token\n",
    "        decoded_texts.append(decoded_text)\n",
    "    return decoded_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0827d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto detectado: ['gagnoon']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_tensor = load_image(\"D:/my_files/universidad/intro_imagenes/EL7007/src/ocr_dataset_train/img/00099.png\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(image_tensor)  # (T, B, C)\n",
    "    logits = logits.log_softmax(2)\n",
    "\n",
    "predicted_texts = decode_prediction(logits, idx_to_char)\n",
    "print(\"Texto detectado:\", predicted_texts)\n",
    "\n",
    "img_np = image_tensor.squeeze().cpu().numpy()  # -> (H, W)\n",
    "# plt.imshow(img_np, cmap='gray')\n",
    "\n",
    "plt.imshow(img_np, cmap='gray')\n",
    "plt.title(f\"Texto detectado: {predicted_texts[0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0a00334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_binarize_image(path, threshold=100):\n",
    "    # Cargar imagen y convertir a escala de grises\n",
    "    image = Image.open(path).convert(\"L\")  # Modo \"L\" es escala de grises\n",
    "\n",
    "    # Convertir a numpy array\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Invertir si el fondo es negro y texto blanco (opcional)\n",
    "    image_np = 255 - image_np\n",
    "\n",
    "    # Aplicar umbral: letras negras (0), fondo blanco (255)\n",
    "    binarized = (image_np < threshold).astype(np.uint8) * 255\n",
    "\n",
    "    # Convertir de nuevo a tensor\n",
    "    image_tensor = transforms.ToTensor()(Image.fromarray(binarized))  # (1, H, W)\n",
    "    return image_tensor.unsqueeze(0)  # Añadir batch -> (1, 1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be603f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto detectado: ['grrte']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAC7CAYAAAB4i/1cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvgklEQVR4nO3deXRUZb7u8V9VpVKpzAQSAhFCiAiCKJOozGDTkQMiYIugIiAt2O1ph6WedVoPCg6cdmBJazeDE9ii9m0VFZuLrQ0ogwiiICoyJyDIEAKBzEPVe/9wUZeY+r2EMhsCfD9rnXXa/WQPtYfaeavIflzGGCMAAAAAAKDeuc/0BgAAAAAAcK5i0A0AAAAAgEMYdAMAAAAA4BAG3QAAAAAAOIRBNwAAAAAADmHQDQAAAACAQxh0AwAAAADgEAbdAAAAAAA4hEE3AAAAAAAOYdAN/EJTpkwRl8sV0bzz5s0Tl8sleXl59btRJ8jLyxOXyyXz5s1zbB0AAMB5/fr1k379+p3pzQBwihh047z13XffyS233CIZGRni8/mkefPmcvPNN8t33313pjftjPjkk0/E5XLJ22+/faY3BQBwnjr+YfTx/4uJiZHmzZtLTk6OPPfcc1JUVBTxsjdt2iRTpkxx9INuEZHPPvtMpkyZIoWFhY6up6H58ccfZcqUKbJhw4YzvSlAg8OgG+elBQsWSJcuXWTJkiUyfvx4mTlzpkyYMEGWLVsmXbp0kXfffbfOy/qf//kfKSsri2g7xowZI2VlZZKZmRnR/AAAnIseffRRee2112TWrFnyhz/8QURE7rnnHunYsaNs3LgxomVu2rRJpk6deloG3VOnTj0vB91Tp05l0A2EEXWmNwA43Xbs2CFjxoyR1q1by/LlyyU1NTWU3X333dK7d28ZM2aMbNy4UVq3bq0up6SkROLi4iQqKkqioiK7lDwej3g8nojmBQDgXDVo0CDp1q1b6L//+Mc/ytKlS2XIkCEydOhQ+f7778Xv95/BLTz/lJaWSmxsbK3p1dXVEgwGz8AWAWcPvunGeefpp5+W0tJSeeGFF2oMuEVEmjRpInPmzJGSkhJ56qmnQtOP/932pk2b5KabbpJGjRpJr169amQnKisrk7vuukuaNGkiCQkJMnToUNm7d6+4XC6ZMmVK6OfC/U13q1atZMiQIbJy5Urp3r27xMTESOvWreVvf/tbjXUcPnxY7r//funYsaPEx8dLYmKiDBo0SL7++ut62lP//7Vt3bpVbrnlFklKSpLU1FSZPHmyGGPkhx9+kOuuu04SExMlPT1dpk+fXmP+yspKefjhh6Vr166SlJQkcXFx0rt3b1m2bFmtdRUUFMiYMWMkMTFRkpOTZezYsfL111+H/Xv0zZs3y29+8xtJSUmRmJgY6datmyxcuLDeXjcAoOEZMGCATJ48WXbt2iXz58+vkZ3svjBv3jy54YYbRESkf//+oX++/sknn4R+ZvHixdK7d2+Ji4uThIQEGTx4cNg/Odu8ebOMHDlSUlNTxe/3S9u2beWhhx4SkZ/umw888ICIiGRlZYXWc+J9fv78+dK1a1fx+/2SkpIio0aNkh9++KHWel544QXJzs4Wv98v3bt3lxUrVoTdL7t375bNmzfXaR/u2rVLhg4dKnFxcZKWlib33nuv/Otf/6q1L/r16yeXXHKJfPnll9KnTx+JjY2VBx98MPScmGeeeUZmzJgh2dnZ4vP5ZObMmXL55ZeLiMj48eNDr/vE+/eaNWvkmmuukaSkJImNjZW+ffvKqlWr6rTdwNmOb7px3vnggw+kVatW0rt377B5nz59pFWrVrJo0aJa2Q033CBt2rSRadOmiTFGXce4cePkH//4h4wZM0auvPJK+fTTT2Xw4MF13sbt27fLb37zG5kwYYKMHTtWXnnlFRk3bpx07dpVOnToICIiO3fulPfee09uuOEGycrKkgMHDsicOXOkb9++smnTJmnevHmd13cyN954o1x88cXypz/9SRYtWiSPP/64pKSkyJw5c2TAgAHy5JNPyuuvvy7333+/XH755dKnTx8RETl27Ji89NJLMnr0aLn99tulqKhIXn75ZcnJyZG1a9dKp06dREQkGAzKtddeK2vXrpXf/e530q5dO3n//fdl7Nixtbblu+++k549e0pGRob893//t8TFxck//vEPGTZsmLzzzjsyfPjwenvdAICGZcyYMfLggw/KRx99JLfffruI1O2+0KdPH7nrrrvkueeekwcffFAuvvhiEZHQ/3/ttddk7NixkpOTI08++aSUlpbKrFmzpFevXrJ+/Xpp1aqViIhs3LhRevfuLV6vVyZOnCitWrWSHTt2yAcffCBPPPGEjBgxQrZu3SpvvvmmPPvss9KkSRMRkdCH/E888YRMnjxZRo4cKb/97W8lPz9fnn/+eenTp4+sX79ekpOTRUTk5ZdflkmTJkmPHj3knnvukZ07d8rQoUMlJSVFWrRoUWOf3HrrrfLpp59afy8R+elf6A0YMED27dsnd999t6Snp8sbb7wR9oNwkZ8+DB80aJCMGjVKbrnlFmnatGkomzt3rpSXl8vEiRPF5/PJ8OHDpaioSB5++GGZOHFi6HesHj16iIjI0qVLZdCgQdK1a1d55JFHxO12y9y5c2XAgAGyYsUK6d69e52OP3DWMsB5pLCw0IiIue6666w/N3ToUCMi5tixY8YYYx555BEjImb06NG1fvZ4dtyXX35pRMTcc889NX5u3LhxRkTMI488Epo2d+5cIyImNzc3NC0zM9OIiFm+fHlo2sGDB43P5zP33XdfaFp5ebkJBAI11pGbm2t8Pp959NFHa0wTETN37lzra162bJkREfPWW2/Vem0TJ04MTauurjYXXHCBcblc5k9/+lNo+pEjR4zf7zdjx46t8bMVFRU11nPkyBHTtGlTc9ttt4WmvfPOO0ZEzIwZM0LTAoGAGTBgQK1tv/rqq03Hjh1NeXl5aFowGDQ9evQwbdq0sb5GAEDDdvy++MUXX6g/k5SUZDp37hz677reF9566y0jImbZsmU1lldUVGSSk5PN7bffXmP6/v37TVJSUo3pffr0MQkJCWbXrl01fjYYDIb+99NPP13r3m6MMXl5ecbj8ZgnnniixvRvvvnGREVFhaZXVlaatLQ006lTpxr30BdeeMGIiOnbt2+N+fv27Wvq8iv99OnTjYiY9957LzStrKzMtGvXrtZ+Ob7M2bNn11jG8d8pEhMTzcGDB2tkX3zxRdjfN4LBoGnTpo3JycmpsZ9KS0tNVlaWGThw4Em3HTjb8c/LcV45/tTThIQE688dz48dO1Zj+h133HHSdXz44YciIvL73/++xvTjD4Kpi/bt29f4Jj41NVXatm0rO3fuDE3z+Xzidv90CQcCASkoKJD4+Hhp27atfPXVV3VeV1389re/Df1vj8cj3bp1E2OMTJgwITQ9OTm51jZ6PB6Jjo4WkZ++zT58+LBUV1dLt27damzjhx9+KF6vN/SthYiI2+2WO++8s8Z2HD58WJYuXSojR46UoqIiOXTokBw6dEgKCgokJydHtm3bJnv37q3X1w4AaFji4+ND9/P6uC98/PHHUlhYKKNHjw7Nf+jQIfF4PHLFFVeEvgnOz8+X5cuXy2233SYtW7assYy6VIcuWLBAgsGgjBw5ssZ60tPTpU2bNqH1rFu3Tg4ePCh33HFH6B4q8tO/oktKSqq13E8++eSk33KL/HSvzcjIkKFDh4amxcTE1Lj3nsjn88n48ePDZtdff32tP9HTbNiwQbZt2yY33XSTFBQUhF53SUmJXH311bJ8+XL+JhznPP55Oc4rxwfTJ6sc0QbnWVlZJ13Hrl27xO121/rZCy+8sM7b+fObuYhIo0aN5MiRI6H/DgaD8uc//1lmzpwpubm5EggEQlnjxo3rvK5IticpKUliYmJC/2zuxOkFBQU1pr366qsyffp02bx5s1RVVYWmn7h/du3aJc2aNav1gJaf77Pt27eLMUYmT54skydPDrutBw8elIyMjLq/OADAWaW4uFjS0tJEpH7uC9u2bRORn/5mPJzExEQRkdCHypdccklE271t2zYxxkibNm3C5l6vV0R+uieKSK2f83q91ge8nsyuXbskOzu71gcE2u8nGRkZNQb9J6rL70PHHd+/4f5k7LijR49Ko0aN6rxM4GzDoBvnlaSkJGnWrNlJ60Y2btwoGRkZoRvtcafrSanaE81P/CR72rRpMnnyZLntttvksccek5SUFHG73XLPPffU+yfG4banLts4f/58GTdunAwbNkweeOABSUtLE4/HI//7v/8rO3bsOOXtOP667r//fsnJyQn7M6fy4QYA4OyyZ88eOXr0aOi9vj7uC8eX8dprr0l6enqtPNKGknDrcblcsnjx4rD30Pj4+HpZT32x/c5zKr8PHd+/Tz/9dOhZLj/X0F47UN8YdOO8M2TIEHnxxRdl5cqVoSeQn2jFihWSl5cnkyZNimj5mZmZEgwGJTc3t8an1Nu3b494m8N5++23pX///vLyyy/XmF5YWFjrG+gz5e2335bWrVvLggULanyy/sgjj9T4uczMTFm2bFmtOpKf77Pjn/B7vV751a9+5eCWAwAaotdee01EJDTAPpX7gvZPwLOzs0VEJC0tzbqM4+v69ttvI16PMUaysrLkoosuUufPzMwUkZ++IT7x2/eqqirJzc2Vyy67zLp+23I3bdokxpga21hfv5+cbP8mJiZy78Z5i7/pxnnngQceEL/fL5MmTar1T6EPHz4sd9xxh8TGxoYqP07V8V8EZs6cWWP6888/H9kGKzweT62/4Xrrrbca1N80H/8k/8TtXLNmjaxevbrGz+Xk5EhVVZW8+OKLoWnBYFD++te/1vi5tLQ06devn8yZM0f27dtXa335+fn1ufkAgAZk6dKl8thjj0lWVpbcfPPNInJq94W4uDgR+enD6RPl5ORIYmKiTJs2rcafQf18GampqdKnTx955ZVXZPfu3TV+5sT7nLaeESNGiMfjkalTp9a6fxtjQr+TdOvWTVJTU2X27NlSWVkZ+pl58+bVWqZI3SvDcnJyZO/evTWq1MrLy2vce38J7XV37dpVsrOz5ZlnnpHi4uJa83HvxvmAb7px3mnTpo28+uqrcvPNN0vHjh1lwoQJkpWVJXl5efLyyy/LoUOH5M033wx9MnuqunbtKtdff73MmDFDCgoKQpVhW7duFZG6PWylLoYMGSKPPvqojB8/Xnr06CHffPONvP7667/o773q25AhQ2TBggUyfPhwGTx4sOTm5srs2bOlffv2NW68w4YNk+7du8t9990n27dvl3bt2snChQvl8OHDIlJzn/31r3+VXr16SceOHeX222+X1q1by4EDB2T16tWyZ8+eeu0pBwCcGYsXL5bNmzdLdXW1HDhwQJYuXSoff/yxZGZmysKFCyUmJib0s3W9L3Tq1Ek8Ho88+eSTcvToUfH5fDJgwABJS0uTWbNmyZgxY6RLly4yatQoSU1Nld27d8uiRYukZ8+e8pe//EVERJ577jnp1auXdOnSRSZOnBj6/WHRokWyYcMGEfnp9wARkYceekhGjRolXq9Xrr32WsnOzpbHH39c/vjHP0peXp4MGzZMEhISJDc3V959912ZOHGi3H///eL1euXxxx+XSZMmyYABA+TGG2+U3NxcmTt3bth7fF0rwyZNmiR/+ctfZPTo0XL33XdLs2bN5PXXXw/ty1/6+0l2drYkJyfL7NmzJSEhQeLi4uSKK66QrKwseemll2TQoEHSoUMHGT9+vGRkZMjevXtl2bJlkpiYKB988MEvWjfQ4J2BJ6YDDcLGjRvN6NGjTbNmzYzX6zXp6elm9OjR5ptvvqn1s8ers/Lz89XsRCUlJebOO+80KSkpJj4+3gwbNsxs2bLFiEiNmi2tMmzw4MG11tO3b98aNSHl5eXmvvvuM82aNTN+v9/07NnTrF69utbP1Udl2M9f99ixY01cXFzYbezQoUPov4PBoJk2bZrJzMw0Pp/PdO7c2fzzn/80Y8eONZmZmTXmzc/PNzfddJNJSEgwSUlJZty4cWbVqlVGRMzf//73Gj+7Y8cOc+utt5r09HTj9XpNRkaGGTJkiHn77betrxEA0LAdvy8e/7/o6GiTnp5uBg4caP785z+Hqjx/rq73hRdffNG0bt3aeDyeWjVZy5YtMzk5OSYpKcnExMSY7OxsM27cOLNu3boay/j222/N8OHDTXJysomJiTFt27Y1kydPrvEzjz32mMnIyDBut7vWff6dd94xvXr1MnFxcSYuLs60a9fO3HnnnWbLli01ljFz5kyTlZVlfD6f6datm1m+fHmte7wxda8MM8aYnTt3msGDBxu/329SU1PNfffdF6rt/Pzzz2ss88T7+XHHf6d4+umnwy7//fffN+3btzdRUVG1fvdYv369GTFihGncuLHx+XwmMzPTjBw50ixZsqRO2w6czVzG1KFjAMAvtmHDBuncubPMnz8/9M/iYPfee+/J8OHDZeXKldKzZ88zvTkAAJxzZsyYIffee6/s2bOH9g/AIfxNN+CAsrKyWtNmzJghbrdb+vTpcwa2qOH7+T4LBALy/PPPS2JionTp0uUMbRUAAOeOn99ry8vLZc6cOdKmTRsG3ICD+JtuwAFPPfWUfPnll9K/f3+JioqSxYsXy+LFi2XixInSokWLM715DdIf/vAHKSsrk6uuukoqKipkwYIF8tlnn8m0adNOW1UbAADnshEjRkjLli2lU6dOcvToUZk/f75s3rxZXn/99TO9acA5jX9eDjjg448/lqlTp8qmTZukuLhYWrZsKWPGjJGHHnqo3vo+zzVvvPGGTJ8+XbZv3y7l5eVy4YUXyu9+9zv5z//8zzO9aQAAnBNmzJghL730kuTl5UkgEJD27dvLf/3Xf8mNN954pjcNOKcx6AYAAAAAwCH8TTcAAAAAAA5h0A0AAAAAgEMYdAMAAAAA4JC6P9Ep8H/1zO1So6BLz4zomcuyacb6WUH4zGVdl/5n7fpcP21JJILWJdrWaHsNtoXaX0X9r0+bJ7LX5sT2RzqfCUb2GmwvwXrMI30J6jHQz1m3dWX2szai+Vy2+SxZpPO5be8bkT7aIpJtiXD7rVm1A8us/ywY6Xtm0Hb+hRfppWNdZoSnie26Cwaq1MwTfX1kKwQAAA0K33QDAAAAAOAQBt0AAAAAADiEQTcAAAAAAA5h0A0AAAAAgEMYdAMAAAAA4JC6P73crT9h1f5wadvTy/Uxv7E89diIx7I2bZkRfr5ge4hyhE/itTzQXRx55q59hbYZ1cT6yrX12Wc69eWdTIRPPbc+ZT3ij6kie0J5pM/UVndZpMfAnOYnjdvmi/QR0hGr76d4R7o823ui7eneeuaK8Kn0JtL3vojmEhGXZTuV69y2hRGXAlgv48j2lznt5zMAADjd+KYbAAAAAACHMOgGAAAAAMAhDLoBAAAAAHAIg24AAAAAABzCoBsAAAAAAIcw6AYAAAAAwCF1rgwzEohwFXrHir1iJZIl6st0WSqPXJYeGGt9VMS1OZHtk5MsVBVpIU2ktUDaxkRaLRdxo07EVXaWuaz7OcIioogr3SIQaVtVxAu1iLBOLOLTwVp7Zp3REkZS8RVZLVjk89mqxiJdpo3lvdY6nwNvYqdvgdZ9Gem9DgAAnBv4phsAAAAAAIcw6AYAAAAAwCEMugEAAAAAcAiDbgAAAAAAHMKgGwAAAAAAhzDoBgAAAADAIXWuDIu8t8mySFuNitHraqwVX8psts132T57cKJGyW1ZX8TtUZZ94kCTlXW3KDvbvist54ID++R0173Z1xbpCzz1+ezngiN9YpGx9gJatsV2skTenRcZdVNsb0aRVXhFXu8X2Xyn+9Pa01iqdxIUfAEAgFPHN90AAAAAADiEQTcAAAAAAA5h0A0AAAAAgEMYdAMAAAAA4BAG3QAAAAAAOIRBNwAAAAAADql7ZZitisfSQ2SvWLKM+av1yG1tNlLWZ60SsvYT2VZmyWyfZ3giXF+Es0Vcv2SZrZ4r5Oy1YKe768kmsuNjn8u2nZGdf9pc9qVZavqsM57eQidjfU+JsDLM+hrq+TyK8Ho0lsxlOUD2rdffp2wVeJGfs5Fxm0jr+DS2+1lkS4z8LKGGDACAcx3fdAMAAAAA4BAG3QAAAAAAOIRBNwAAAAAADmHQDQAAAACAQxh0AwAAAADgEAbdAAAAAAA4pM6VYS6X35LqVUNlJSVq5jF6dZY7oH8e4E5qrG9KWUXYyd+t/VKdZefOXDVbumyZmhUePaJmwYBeA5MUn6xmPl+Mml162WVq1ndAfzVrmZ2lZuLz6pmtmy0YUKOAklUF9B44j1c/Fb1efZ/Yqn+MhD8XREQClm1xuy2fRUXYhqTtExGRgNGvH7dHv0a8Yjl26udptlow24vTj48JlOvLjLK9b+j7ORgoVbPqoH5eRntj9dV59NdXdaxYzbyJSfoyK/T5JErZZx79uFUU56uZzx+tZmWV+jFwR1neS92W+jXL+RDlstQe2s4jS/WXVOvXiMurv3ZXUDmnbeuybGIw0mvV8rbh9thutfoycWrs72ENh7GdmwCAcxLfdAMAAAAA4BAG3QAAAAAAOIRBNwAAAAAADmHQDQAAAACAQxh0AwAAAADgEAbdAAAAAAA4xGXq2F0RDCxUM2Op8PEYS1VKdJxljXpFzNf/1mu81n62Nuz0VStXqfPs2FmgZn7LJrbKvkDNGiXrtWaFhwrVrLJCr7LKLzikZq4ovcKn39V6ndjoW25Ss+YXX6RmUlmmZ1pFkVs/piJ6TU9luV7LZKv+ivH71MzWLFNRYasas1UG6ccgxm+psrJWf+lslWilZeErtwJV+v5KTGyiZgUH9qpZ46bN1MxY9mVlVZWa+eIbqZmt7+nQfn07mzTRt9NW4/XFsiVq9vXX69WsUXJi2OlXDxygzuOL068RfyP9+NiuH3tmY6kFsyzTVOjvDabadv3onwFXWCrRxBX+/hPts9SM+Wz3HlvlmV6BWVauZ7b3qbhYvVbP5b5e35ZzwNlS8dVQUDUGRM72fsO1hdOBb7oBAAAAAHAIg24AAAAAABzCoBsAAAAAAIcw6AYAAAAAwCEMugEAAAAAcAiDbgAAAAAAHGLp86qpokyv9/FYqmU8cZaqJKPX9OR+vk7N3nnjHTX7ZsPGsNOrq/WKm/ZtM9Rs2PXD1eySzpeq2QUXtFAzV1yKmskxvXZm2ZJ/q9kb/+fvavbPhYvU7Pstm9XsxptGqdnAEXqVTXlhftjpUV69Psqt1YyJiNetnyfRPr1uR1yRVZT5LNsiMbbqL/0akWq9Mihoqf4yRq9YsjUbxfkTwgd+fZ8Ey4vUrHFTvR7PVi0VrNLro3z+eH2+Mv06cPv1Y97YUtVXabm2olNS1WzJh/p19+67H6lZl86tw04fMug/1Hl8jdLULFhSqGZun+Wt3FInGLCcl5UV4WvnRERiLBVrrhi97s3ls1xbRt8Wnz9Jny+o1IlV6deVlOuvzfpRtFuvlfHb3otsLzuov+5zwemsBTtban8i3SdUHgF29X1tcV2hPvFNNwAAAAAADmHQDQAAAACAQxh0AwAAAADgEAbdAAAAAAA4hEE3AAAAAAAOYdANAAAAAIBD6lwZFqzWq4v8sXr1j61F6fCOHWr27FPPqln+vvCVVCKiNkHlDPy1OsugoYPVLLtPD31dLkudk63PKb9Qz1L0OrH+o0fqWc7VavbX2TPVbMZzev3a4WOz1KxjJ70uLb31heGD8mJ1HrG1MtiqeKr0E6zo0H59tkq9TigmJkbNjh37Uc1Smuh1VdGN9Bol6ydf1UodkohI0HL+VSmZy1LbZmxbor9VlB3Yp2b+VL0CS9z6Mgv271WzgOV1JyXq+9mf2ETflmK92swV0K9lv+UdNMYTvp6tskQ/pr4qvT7KbalYqyw5pmZRPr0mzhOtX1v+qEQ1s9V7BY/o21JkySrK9WPgj9NfQ0IjZTsTLdtve8Op0t+nqixVY55ovZrNHR1ZfeHZglqwU2N7DdSJAcC5iW+6AQAAAABwCINuAAAAAAAcwqAbAAAAAACHMOgGAAAAAMAhDLoBAAAAAHAIg24AAAAAABxS58owr0evQ5HoODWq2q/Xey1f9qma7dyap2ZNLPVLrdtlhZ1+3fBh6jwtuur1V1Kt19hUlBxRs/IKvRYoqWkrNTNHCtTMFWU5Bk2S1Kjfr/uq2bfbvlOzuAS9osgdpX9eU11UGHZ6VIK+jbbPf4r36TVdq1euUrPPVn6mZnk7c9WsvEw/5tE+n5q1vrC1mnW/8gpL1l3NGmeFP59PpqrgYNjp+QfCTxcReXXePDXzevS3irYXXaRml156mZr9e8lSNdu2bZuaHSk8qmaXXdZZzVpe0ErN1q5eo2YFB/V91rJ5UzUrOVoUdvrjUx5V54mO1c+vHn17qdmgUXqdoIilWs5SBXn4oF7btmbl52q2YsknanZgj14v54u23I7c+namZ4Q/Brbrqkevq9Qs8QL9mHq9+nZUV+nv+W6PXrHmstVLNiDUgp0eTtSJnQ0a0ms7G86/SPdXpK/tdB+fhnIdnO79fLo1lOvubNlfvxTfdAMAAAAA4BAG3QAAAAAAOIRBNwAAAAAADmHQDQAAAACAQxh0AwAAAADgEAbdAAAAAAA4pM6VYdFeb0Qr2L9Xr3v6fNVqNUtpZKmXsjxavkvXLmGnt7jycn157io1ChQfVjNfsr6N0aJX/0hArzxyxeufg5iApXbGlKhZh+7d1Gx6u2fUrLpKr8dJbNxMzUSrwDH68tb8e4mavThrjpp98flONWveVD9nO1uqpdLT9MqgH/fr5/P6NV+q2adLlqvZFVfqx2fM2DFq1r5nDzXzJoav1Us8qp8nP+zIU7Mt3+vZ5lbfqtmbr87X59tyTM2aNdfr8fLzA2o2+Nf/oWZHD+nX8u7cPDWLslRqlBeXqpmpUt5eLdeBO1p/3fv36nVb4ra8Pwcq1Wjd53pV2t/mvaZmX67eqGaNE2PVrEObdmqW2rixmh04pF93a1Z9EXb6J8vCTxcRGTBwnZqNvOkGNbvosg5qFhWXqGYS0OvEgLo6G2p1GkoF0S9xLtdENaTj01BqwZxg234nzpNzeX/ZnA3X3In4phsAAAAAAIcw6AYAAAAAwCEMugEAAAAAcAiDbgAAAAAAHMKgGwAAAAAAh9T56eXisYzPLU/H3Wd5evme3T+omd/r19dneVpdpy6dwgdB/QnlUqo/1Tjg1ddVaQrVzOfSn15eFihSs9ho/YnoLq9+uMoq9G0J6g97ltjEeH19oj9JubykQM1iosM/vfir1frThBe8/baabd+iP6H88s5ZajZh7Hg1uypnsJqJ33LuVZWp0aoP3lezl155SZ/vE/1Jyl63fgx+69O3s+Wll4SdHt+4iTpP5w4d1aykoFDNghX6tZXdspWaPfawfnwapehPsv54if6k+66XdVKzjC7d1WzEdcPV7I3X/qZmixd9oGYtWmeGnX7XPXep86S3aqlm0tjS6FCtH4Nt321Ss4XvLVSzzz/Tn1Ce3TJdze69U3993a8ZomZWx/T3m48Xvht2+rz589R5lnykP7XdcsnJ2Hj9mstod6GaBSzXiCf6/P3s+2x78ix+4sSTkhvSuRDp6zvdT6w+nSLd/tP9VOr6Pjcb0nE7l6+78+WaO3/v9gAAAAAAOIxBNwAAAAAADmHQDQAAAACAQxh0AwAAAADgEAbdAAAAAAA4hEE3AAAAAAAOqXtlmO1x7iUlanT06FE1q6ysULN4X/jaKRGR5s2aq1l2Z6X2qOiIOo8ketUo2qtvx56jeWrWLEmv1IlVKrVERIoqDqqZuPQumwRfsj6f5TAbCVrm0495TGycPlsg/HxfrdUrw9au0rP0NL0+avRNN6rZVYN+pWYSpx9zOZyvZwl6xVrPawaqWUVVuZrNmTNbzTas+1LNPm97sZplNEkLO92T0kidR6r1eoUqS+WR8ernUJcuXdQsu29vfVuKi9Xoxrt+r89Xqr+nSLFe1efP0qvnikv197f8w/pxbVERPktr2lSdR5IstWCF+nupNElRo+WWirUVy1aoWfs2GWo2YewENeveo4ea2e4V1tdnqUsbeHX46+7QoUPqPLNfekPNli/V68Q6d+6kZhktL1Azj62HTBperQlwLtcTnYxtO8+XaqP6ci6/Niecz9fd+YBvugEAAAAAcAiDbgAAAAAAHMKgGwAAAAAAhzDoBgAAAADAIQy6AQAAAABwCINuAAAAAAAcUvfKsHK9Mkh8en1URYVe4WOrqzp67LCatW2brW9LqVI7k5asz+O11bnorzsxKUHNSkSvxom2VMQkWPZlqalUs8IyfX/F+RLVzCvRaiZVAT1z65Vb+Vt3hJ2+5hO9nqhRvE/N2l18oZpdPVSv6Qq49Iood1CvepIkS11ItaXWKFr/DOuy7pepWfzf9RqyKEvV0L8//EjNrukfvi4tMU2vNXIF9e33RunnSUycXoHXrddVama7tsSv12YEg3qdmDvWcj6Lfo5JYaGeRevnermlcS82OXzNlSva8rZbZak8i4lRo4pdu9RsxUf/VrO0eL+aJXr11335pUo1o4iI37KfbfeR9FQ9q6rWs7jw+3nY4OvUWdatWatmW3du1+dbvU7Nrh7QT82i08NX+ImISJXlPQUAThOqpXCmnC81fXzTDQAAAACAQxh0AwAAAADgEAbdAAAAAAA4hEE3AAAAAAAOYdANAAAAAIBDGHQDAAAAAOCQuleG+fRqGXHptUaxsXqdUHW1XknVLD1dzfbu/UHflqC2TP3x8IV78tQs+QK9xqaqQq+/8fv0eh+f5bOOI8V69Zc/Vq8oS/ZbKmks1WwSsNSlRVmqf47plVt7du4OO734sF6NU1akVyUlxOo1ahKtb7/Hcs6WVOj7ubS0TM1SYvT6NU9sspolNgpfayQickGLDDXbsvF7NTtaqu+z/P0Hw29H9kXqPCagXyPBoJ5VG/38CliuO3FZMrdlfZZlWlrbRCyvTxL02rYYv/4e5ra8g5aUhz+PCg7r516TGEvlmVdf2eH8Q2oWbXt/TtDPy/IivfZw5nPPqVmwWt/PZWX6+0Z8rH4MKsv1ukS3J/xBLyrRq+X2/LBHzYylKXHf3n0RZZlxluPqi6wOBQCAk4m0cgv1j2+6AQAAAABwCINuAAAAAAAcwqAbAAAAAACHMOgGAAAAAMAhDLoBAAAAAHAIg24AAAAAABxS98owt6VayqNXSzVv3lzN4uL0Sqf9+w+oWaxPX9+277eGnd6mRX91nuRGtrotva4q0atX43hFr4gpKytUs6SYZDVzuy3VWZYapWCF3oHjdllOAa9XzxL0SrTqyvDrc4t+DiUn6lVcCbF6JlF6nZNU6zVDcdH6MuN8KfoyK/TKIxG9lsHr1c+HKI9+DGJi9P1cXlWqZp4oZV979WMQCFSrWVCt4hMJBvXKMFtmaxOzsbSJ2ZdZZemCslSiFR07pm+LpYnD51Xep2zbGGW55ixcRt+Q0hL9PIm2nJeVFfr1c2Cf/v5cXGw5Ly0daweC+WpWVaVvS2pak/DbUapXhrVu01rNKqr1Kr7mLZupWVq65T6SkKxnlXqVIgAAODfwTTcAAAAAAA5h0A0AAAAAgEMYdAMAAAAA4BAG3QAAAAAAOIRBNwAAAAAADmHQDQAAAACAQ+pcGWYq9RoVl1+v/mp9Ybaate/QQc2++nydmjVq3FjNPlj4z7DT78hsoc4Te3EbNZOKEjVyW9qQyoN6bY4/1lJJZanwKjqo1/REafVEIuJvpNfclB44qGZHDu5Ws4wOl6lZk9Tw1TnRPkv9VVmZmhUetlTqlOpVQqZSX6bLr1clibtKjapK9Mowr+h1TxWl+rbs+3GfPl+Fft1FeS1VY7HKvi7Tz+eA0SvDApbKMGP0C8FYqrjE2DI9irgyzLKdYqncstWl2Vr1fEq1octl6RnzWKoZLdvRtGlTNbPVzlVW6Od6VpZeq/XQHx/S15eu10SK5X3KWN7frNerCb9fCvL15aWk6veQ8ir9Gvcn6vc6SdbrCwOFeh2aJy6ymrhzgXYtGNt7AwCgzng/bTj4phsAAAAAAIcw6AYAAAAAwCEMugEAAAAAcAiDbgAAAAAAHMKgGwAAAAAAhzDoBgAAAADAIXWuDKuq1qtloqv1OqTotCZqNnDQr9Vsx9Ztanb02DE127N6Tdjp3a64Qp2nT9N0NZNGSWrkcSXomaWaSQr1GiiJ1Q9JQoqliidKrwXa8/0mNXvmqelqdjj/sJrdOfH3anZFj55hp1/YRq9mW7Z0pZrtztOry3Zv/F7NWl7VXc3EUoFXWVigZtHJ+vkglkq0Xbnr1ayoqEjNSkr08+jK7vo53ahxIyXRa6eCLr1WIuiy1G1Z5nPZO7wiiiyrs/NYPmO01B4mJSWqWdCyWyoqwldP+XyW+itbB1lArxqz1Tbaqhk/+nC5mlVV7VCz3LxdanZxm7ZqJiV6laLLul8sxy4ufFVX4+b6vUfK9e2QAr0yTFK060pEig+pkcdnOa4Rn9Cnl612xlqDh3oT6X6mMggAzjy+6QYAAAAAwCEMugEAAAAAcAiDbgAAAAAAHMKgGwAAAAAAhzDoBgAAAADAIQy6AQAAAABwSJ0rwzxR+vi8slivPIr2x6vZ5b2uUjNbZdj777ynZiVK28vCD/6pzrP/kF718h9Dr1Wz+Asy1Ey8cXpmfHpWban2qKxWo41f6dU/s16Yo2Zr1+r7+VcDuqlZ02ZpaiYp4Wu1+g8coM7y1fqv1Gzvjz+q2QsvvKBmj7ZooWbuRL0GKjrNUs1WrVeNFe3MU7M333xTzY4ePapmSZbKumEjhqmZL6NZ+MBYOq6iLHU0lszYPrpzW5YZYf2NvWHJFup1aVJcqEbVQb0usdjSPFWpnCvRftv1X6lntn0Zoy/z2mHXqdnmLVvVbO+e/Wr24isvqdkjF1ygZkmplveNdEsW0GspxecJO7lsd546y7PPzVCzH/b+oGb33ne3mjVv2VTN4ptYqgaN5Zifp87naqyGUr/mREWcbb6GdOycOAYN6fWh4TqfrzvNuXQ/4JtuAAAAAAAcwqAbAAAAAACHMOgGAAAAAMAhDLoBAAAAAHAIg24AAAAAABzCoBsAAAAAAIfUuTLMVsXjsdQJBUr1OjFPShM1GzVpgppFefXNXv/lhrDTv/jie3Web7fsVLNNW/VKnawLL1SzJk3019YoQa+rytu1S83WWWq1Vq/TX5+t0mn8hGFqduuYW9UssbmlLq0qfL3PZd0uVWcZceMINVu1YpWafb72CzWbNXuWmjXP0Lc/K7u1mu3Zt0fNvt+ySc1WrFytZpkt9W25ul9/NWvfSd+f4o8OO9nk5+vzeC0nisdS2RBpZuv+smZ6ZK8M0yv3KsuL1ax5y3RL5lWz/MLw+/pfSz5U54mO1pfX86oeahbfVK+ranPJxWo2aOhgNfvoXx+p2ep1etXgs3+ZoWYZzfU6sUsu7aBmXn/4WjARkR8PhK8UXL5Sr1Fc+skWNevcVa8uS2qsV3/Fp6WqWbBcvw+69UN+1nCi5iYSp7tapqHUe4k0zHqcU3E+HzvUn4byXnS2aCh1XOfLseGbbgAAAAAAHMKgGwAAAAAAhzDoBgAAAADAIQy6AQAAAABwCINuAAAAAAAcwqAbAAAAAACHuEwdn/teWfyOmkXHxalZyaHDahaX2EhfYbVlsyr06p8N69aHnf7vj5ao8xzML1Cz7zbpVVzFpfo2lpWokfgsRW3pzWPVLCbOr2bNLLVT/zF0iJr1+/VAfWO8MWpUfkSvnoqJUbbTr58nEtCjD9/4h5ot/Xipmm3atFnNSkoq9RVamgui/PrnVJXVQTXr2KGdml2T82s1GzL6Jn1jysv1LC78eVS1f586y6w5esXailUr1SxWWZeIyMNTp6hZdmdL5VlVlRqZoH6yuDx6tZS4LReeSz+uhXv1mrjnn39ezRYtCr/PjH6aSIx+ycmz0x9Vsy5XXKnP6LLsE2/4ajkRkTUffaxmtjqxjV9vVLPi4lI1yz+kRpKsNzBKWUX46Re00N9vbh5zs5oNGnyNmnmVKj4RkYoK/U3fF2M7Ly0nhPsGPTuHnS/1MeGc7dVfNufCcW0ox8eJfdlQXptTTuf515D25dl+3TWkfflL8U03AAAAAAAOYdANAAAAAIBDGHQDAAAAAOAQBt0AAAAAADiEQTcAAAAAAA5h0A0AAAAAgEPqXBkGAAAAAABODd90AwAAAADgEAbdAAAAAAA4hEE3AAAAAAAOYdANAAAAAIBDGHQDAAAAAOAQBt0AAAAAADiEQTcAAAAAAA5h0A0AAAAAgEMYdAMAAAAA4JD/B58jTReVsLiRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path = \"D:/my_files/universidad/intro_imagenes/EL7007/src/ocr_dataset_test/corte.png\"\n",
    "image_tensor = load_and_binarize_image(image_path).cuda()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    logits = model(image_tensor)\n",
    "    logits = logits.log_softmax(2)\n",
    "\n",
    "predicted_texts = decode_prediction(logits, idx_to_char)\n",
    "print(\"Texto detectado:\", predicted_texts)\n",
    "\n",
    "# Load original image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert image_tensor to CPU and NumPy array for display\n",
    "# Assumes image_tensor shape is (1, C, H, W)\n",
    "tensor_image = image_tensor.squeeze().detach().cpu()  # shape: (C, H, W) or (H, W)\n",
    "if tensor_image.ndim == 3:\n",
    "    tensor_image = tensor_image.permute(1, 2, 0)  # (H, W, C)\n",
    "\n",
    "# Plot original and tensor image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Tensor image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(tensor_image, cmap='gray' if tensor_image.ndim == 2 or tensor_image.shape[2] == 1 else None)\n",
    "plt.title(f\"Detected: {predicted_texts[0]}\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9df2f",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8592cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = OCRDataset(\"ocr_dataset_test/labels.csv\", \"ocr_dataset_/img\")\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b30c515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader, device, idx_to_char):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, label_lengths in tqdm(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)  # [T, B, C]\n",
    "            log_probs = log_softmax(logits, dim=2)\n",
    "\n",
    "            # Predicción y decodificación\n",
    "            pred_texts = decode_prediction(log_probs, idx_to_char)\n",
    "\n",
    "            # Reconstrucción de etiquetas reales\n",
    "            labels = labels.cpu().tolist()\n",
    "            lengths = label_lengths.cpu().tolist()\n",
    "\n",
    "            gt_texts = []\n",
    "            i = 0\n",
    "            for l in lengths:\n",
    "                gt_texts.append(\"\".join([idx_to_char.get(c, \"\") for c in labels[i:i+l]]))\n",
    "                i += l\n",
    "\n",
    "            for pred, gt in zip(pred_texts, gt_texts):\n",
    "                if pred == gt:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be11c64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:51<00:00, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 3.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = calculate_accuracy(model, val_loader, device, idx_to_char)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f4ff6",
   "metadata": {},
   "source": [
    "### WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bee9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer(model, dataloader, device, idx_to_char):\n",
    "    model.eval()\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, label_lengths in tqdm(dataloader, desc=\"Calculating WER\"):\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            logits = logits.log_softmax(2)\n",
    "\n",
    "            pred_texts = decode_prediction(logits, idx_to_char)\n",
    "\n",
    "            label_offset = 0\n",
    "            for length in label_lengths:\n",
    "                true_text = ''.join([idx_to_char[idx.item()] for idx in labels[label_offset:label_offset + length]])\n",
    "                ground_truths.append(true_text)\n",
    "                label_offset += length\n",
    "\n",
    "            predictions.extend(pred_texts)\n",
    "\n",
    "    return wer(ground_truths, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca2a6e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating WER: 100%|██████████| 625/625 [20:17<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation WER (Word Error Rate): 96.80%\n"
     ]
    }
   ],
   "source": [
    "val_wer = calculate_wer(model, val_loader, device, idx_to_char)\n",
    "print(f\"Validation WER (Word Error Rate): {val_wer * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdc261",
   "metadata": {},
   "source": [
    "### CER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6badecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cer(model, dataloader, device, idx_to_char):\n",
    "    model.eval()\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, label_lengths in tqdm(dataloader, desc=\"Evaluando CER\"):\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            logits = logits.log_softmax(2)\n",
    "\n",
    "            pred_texts = decode_prediction(logits, idx_to_char)\n",
    "\n",
    "            label_offset = 0\n",
    "            for length in label_lengths:\n",
    "                true_text = ''.join([idx_to_char[idx.item()] for idx in labels[label_offset:label_offset + length]])\n",
    "                ground_truths.append(true_text)\n",
    "                label_offset += length\n",
    "\n",
    "            predictions.extend(pred_texts)\n",
    "\n",
    "    return cer(ground_truths, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b36c97b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando CER: 100%|██████████| 625/625 [00:54<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER (Character Error Rate): 30.47%\n"
     ]
    }
   ],
   "source": [
    "val_cer = calculate_cer(model, val_loader, device, idx_to_char)\n",
    "print(f\"Validation CER (Character Error Rate): {val_cer * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c2ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
